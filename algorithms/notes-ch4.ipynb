{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general approach to optimization of a multivariate function is local descent: use a local model to determine which direction to move in, and then determine a step size.\n",
    "\n",
    "### Descent direction methods\n",
    "\n",
    "Start with an initial value for $x$ and find the direction which minimizes objective value $f\\left(x\\right)$ according to some local model. Iteratively repeat this procedure until some convergence threshold is obtained. The descent direction is often determined using local information such as gradients or Hessians. The update usually goes as $$ x^{\\left(k+1\\right)} = x^{\\left(k\\right)} + \\alpha^{\\left(k\\right)} d^{\\left(k\\right)} $$ where $\\alpha^{\\left(k\\right)}$ and $d^{\\left(k\\right)}$ are the step size and step direction, respectively. Methods differ in how to choose these quantities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}